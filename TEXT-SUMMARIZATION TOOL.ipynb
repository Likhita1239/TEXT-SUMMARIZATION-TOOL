{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "216a57d7-46a8-45f6-869c-53162bb284ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sumy\n",
      "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting breadability>=0.1.20 (from sumy)\n",
      "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests>=2.7.0 in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from sumy) (2.32.3)\n",
      "Collecting pycountry>=18.2.23 (from sumy)\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: nltk>=3.0.2 in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from sumy) (3.9.1)\n",
      "Requirement already satisfied: chardet in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from breadability>=0.1.20->sumy) (4.0.0)\n",
      "Requirement already satisfied: lxml>=2.0 in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from breadability>=0.1.20->sumy) (5.2.1)\n",
      "Requirement already satisfied: click in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from click->nltk>=3.0.2->sumy) (0.4.6)\n",
      "Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
      "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.3 MB 1.4 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 0.8/6.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.0/6.3 MB 1.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.3/6.3 MB 1.5 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 1.8/6.3 MB 1.5 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.4/6.3 MB 1.6 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.1/6.3 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 3.9/6.3 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 4.7/6.3 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.5/6.3 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.3 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 2.3 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: breadability, docopt\n",
      "  Building wheel for breadability (setup.py): started\n",
      "  Building wheel for breadability (setup.py): finished with status 'done'\n",
      "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21738 sha256=62aac52128c33e25234eebdb4b435c9a52ec029271f77e693b58fa1eadb7413d\n",
      "  Stored in directory: c:\\users\\kiit0001\\appdata\\local\\pip\\cache\\wheels\\32\\99\\64\\59305409cacd03aa03e7bddf31a9db34b1fa7033bd41972662\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13775 sha256=4d9e0ad1372aa9887a4127d42ecd9841118e30fb8b5f67bef8d071a345c73e28\n",
      "  Stored in directory: c:\\users\\kiit0001\\appdata\\local\\pip\\cache\\wheels\\1a\\bf\\a1\\4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
      "Successfully built breadability docopt\n",
      "Installing collected packages: docopt, pycountry, breadability, sumy\n",
      "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-24.6.1 sumy-0.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KIIT0001\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install sumy\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download required tokenizer for sumy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eed6cac-f4ac-4c78-a346-4af7c2988a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee44c17e-b581-4b1b-b7b1-05dd65a34625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, sentence_count=3):\n",
    "    \"\"\"\n",
    "    Summarizes the input text using LSA summarization technique.\n",
    "    \n",
    "    Parameters:\n",
    "    - text: String, the article to summarize\n",
    "    - sentence_count: Int, number of sentences in the summary\n",
    "    \n",
    "    Returns:\n",
    "    - summary: String, the summarized output\n",
    "    \"\"\"\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer = LsaSummarizer()\n",
    "    summary = summarizer(parser.document, sentence_count)\n",
    "    return \" \".join(str(sentence) for sentence in summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ccd1afd-511b-4682-ac66-958fdfe388bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Artificial intelligence (AI) is transforming industries and redefining the way we interact with technology.\n",
    "From healthcare to finance, AI is being used to automate tasks, improve efficiency, and generate insights from data.\n",
    "Machine learning, a subset of AI, enables systems to learn and improve from experience without being explicitly programmed.\n",
    "Natural language processing (NLP), another key area, allows machines to understand and respond to human language.\n",
    "As AI continues to evolve, ethical considerations and regulations are becoming increasingly important.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b75a988-b4ff-4f45-9471-083f446e18e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "NLTK tokenizers are missing or the language is not supported.\nDownload them by following command: python -c \"import nltk; nltk.download('punkt')\"\nOriginal error was:\n\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\KIIT0001/nltk_data'\n    - 'C:\\\\Users\\\\KIIT0001\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\KIIT0001\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\KIIT0001\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\KIIT0001\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sumy\\nlp\\tokenizers.py:172\u001b[0m, in \u001b[0;36mTokenizer._get_sentence_tokenizer\u001b[1;34m(self, language)\u001b[0m\n\u001b[0;32m    171\u001b[0m     path \u001b[38;5;241m=\u001b[39m to_string(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m%\u001b[39m to_string(language)\n\u001b[1;32m--> 172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mload(path)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mLookupError\u001b[39;00m, zipfile\u001b[38;5;241m.\u001b[39mBadZipfile) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:823\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path_\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m switch_punkt(fil)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path_\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunkers/maxent_ne_chunker\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:678\u001b[0m, in \u001b[0;36mswitch_punkt\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PunktTokenizer \u001b[38;5;28;01mas\u001b[39;00m tok\n\u001b[1;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tok(lang)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1743\u001b[0m PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\KIIT0001/nltk_data'\n    - 'C:\\\\Users\\\\KIIT0001\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\KIIT0001\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\KIIT0001\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\KIIT0001\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m summary \u001b[38;5;241m=\u001b[39m summarize_text(input_text, sentence_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Text:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, input_text)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSummary:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, summary)\n",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m, in \u001b[0;36msummarize_text\u001b[1;34m(text, sentence_count)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msummarize_text\u001b[39m(text, sentence_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Summarizes the input text using LSA summarization technique.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    - summary: String, the summarized output\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     parser \u001b[38;5;241m=\u001b[39m PlaintextParser\u001b[38;5;241m.\u001b[39mfrom_string(text, Tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     13\u001b[0m     summarizer \u001b[38;5;241m=\u001b[39m LsaSummarizer()\n\u001b[0;32m     14\u001b[0m     summary \u001b[38;5;241m=\u001b[39m summarizer(parser\u001b[38;5;241m.\u001b[39mdocument, sentence_count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sumy\\nlp\\tokenizers.py:160\u001b[0m, in \u001b[0;36mTokenizer.__init__\u001b[1;34m(self, language)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_language \u001b[38;5;241m=\u001b[39m language\n\u001b[0;32m    159\u001b[0m tokenizer_language \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLANGUAGE_ALIASES\u001b[38;5;241m.\u001b[39mget(language, language)\n\u001b[1;32m--> 160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentence_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sentence_tokenizer(tokenizer_language)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_word_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_word_tokenizer(tokenizer_language)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sumy\\nlp\\tokenizers.py:174\u001b[0m, in \u001b[0;36mTokenizer._get_sentence_tokenizer\u001b[1;34m(self, language)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mload(path)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mLookupError\u001b[39;00m, zipfile\u001b[38;5;241m.\u001b[39mBadZipfile) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(\n\u001b[0;32m    175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNLTK tokenizers are missing or the language is not supported.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"Download them by following command: python -c \"import nltk; nltk.download('punkt')\"\\n\"\"\"\u001b[39;00m\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal error was:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[0;32m    178\u001b[0m     )\n",
      "\u001b[1;31mLookupError\u001b[0m: NLTK tokenizers are missing or the language is not supported.\nDownload them by following command: python -c \"import nltk; nltk.download('punkt')\"\nOriginal error was:\n\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\KIIT0001/nltk_data'\n    - 'C:\\\\Users\\\\KIIT0001\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\KIIT0001\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\KIIT0001\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\KIIT0001\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "summary = summarize_text(input_text, sentence_count=2)\n",
    "print(\"Original Text:\\n\", input_text)\n",
    "print(\"\\nSummary:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69906584-1b6a-41e3-bc3f-44079acd5f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KIIT0001\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\KIIT0001\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')  # This line is essential for your error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e94e918-3326-41f6-88ab-8239f514e0e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " \n",
      "Artificial intelligence (AI) is transforming industries and redefining the way we interact with technology.\n",
      "From healthcare to finance, AI is being used to automate tasks, improve efficiency, and generate insights from data.\n",
      "Machine learning, a subset of AI, enables systems to learn and improve from experience without being explicitly programmed.\n",
      "Natural language processing (NLP), another key area, allows machines to understand and respond to human language.\n",
      "As AI continues to evolve, ethical considerations and regulations are becoming increasingly important.\n",
      "\n",
      "\n",
      "Summary:\n",
      " Artificial intelligence (AI) is transforming industries and redefining the way we interact with technology. Machine learning, a subset of AI, enables systems to learn and improve from experience without being explicitly programmed.\n"
     ]
    }
   ],
   "source": [
    "summary = summarize_text(input_text, sentence_count=2)\n",
    "print(\"Original Text:\\n\", input_text)\n",
    "print(\"\\nSummary:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452eb7e0-037c-4191-8440-737574679e83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
